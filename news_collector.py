"""
ë‰´ìŠ¤ ìˆ˜ì§‘ ìë™í™” ìŠ¤í¬ë¦½íŠ¸
- ë„¤ì´ë²„ ë‰´ìŠ¤, Google News RSS ë“±ì—ì„œ íŠ¸ë Œë“œ ê¸°ì‚¬ ìˆ˜ì§‘
- í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§ ë° ìš°ì„ ìˆœìœ„ ì§€ì •
"""

import requests
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime
import json
from typing import List, Dict

class NewsCollector:
    def __init__(self):
        self.keywords = [
            "ì‚¼ì„±", "í˜„ëŒ€", "ì¿ íŒ¡", "ë°°ë‹¬", "ë¶€ë™ì‚°",
            "ì£¼ì‹", "ê²½ì œ", "ì •ì±…", "ì†í¥ë¯¼", "AI"
        ]
        self.collected_news = []

    def fetch_naver_news(self, keyword: str, max_results: int = 10) -> List[Dict]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¥¼ í†µí•œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        # ì‹¤ì œ êµ¬í˜„ ì‹œ ë„¤ì´ë²„ API í‚¤ í•„ìš”
        url = f"https://openapi.naver.com/v1/search/news.json"
        headers = {
            "X-Naver-Client-Id": "YOUR_CLIENT_ID",  # ì‹¤ì œ í‚¤ë¡œ êµì²´
            "X-Naver-Client-Secret": "YOUR_CLIENT_SECRET"
        }
        params = {
            "query": keyword,
            "display": max_results,
            "sort": "sim"  # sim: ì •í™•ë„ìˆœ, date: ë‚ ì§œìˆœ
        }

        try:
            response = requests.get(url, headers=headers, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                articles = []
                for item in data.get('items', []):
                    articles.append({
                        'title': self._clean_html(item['title']),
                        'description': self._clean_html(item['description']),
                        'link': item['link'],
                        'pub_date': item['pubDate'],
                        'keyword': keyword,
                        'source': 'naver'
                    })
                return articles
        except Exception as e:
            print(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜ ({keyword}): {e}")
        return []

    def fetch_google_news_rss(self, keyword: str) -> List[Dict]:
        """Google News RSSë¥¼ í†µí•œ ë‰´ìŠ¤ ìˆ˜ì§‘ (API í‚¤ ë¶ˆí•„ìš”)"""
        url = f"https://news.google.com/rss/search?q={keyword}&hl=ko&gl=KR&ceid=KR:ko"

        try:
            feed = feedparser.parse(url)
            articles = []
            for entry in feed.entries[:10]:
                articles.append({
                    'title': entry.title,
                    'description': entry.get('summary', ''),
                    'link': entry.link,
                    'pub_date': entry.get('published', ''),
                    'keyword': keyword,
                    'source': 'google_news'
                })
            return articles
        except Exception as e:
            print(f"Google ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜ ({keyword}): {e}")
        return []

    def scrape_article_content(self, url: str) -> str:
        """ê¸°ì‚¬ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘"""
        try:
            response = requests.get(url, timeout=10, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            soup = BeautifulSoup(response.content, 'html.parser')

            # ì¼ë°˜ì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ íƒœê·¸ ì‹œë„
            article_body = None
            for selector in ['article', '.article_body', '#articleBodyContents', '.news_end']:
                article_body = soup.select_one(selector)
                if article_body:
                    break

            if article_body:
                # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ì œê±°
                for tag in article_body(['script', 'style', 'iframe']):
                    tag.decompose()
                return article_body.get_text(strip=True, separator='\n')

            return ""
        except Exception as e:
            print(f"ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
            return ""

    def collect_all_news(self) -> List[Dict]:
        """ëª¨ë“  í‚¤ì›Œë“œì— ëŒ€í•´ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_articles = []

        for keyword in self.keywords:
            print(f"ğŸ” '{keyword}' í‚¤ì›Œë“œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")

            # Google News RSS ì‚¬ìš© (API í‚¤ ë¶ˆí•„ìš”)
            articles = self.fetch_google_news_rss(keyword)
            all_articles.extend(articles)

            # ë„¤ì´ë²„ API ì‚¬ìš© ì‹œ (ì£¼ì„ í•´ì œ)
            # articles = self.fetch_naver_news(keyword, max_results=5)
            # all_articles.extend(articles)

        print(f"âœ… ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_articles

    def filter_trending_news(self, articles: List[Dict], min_relevance: float = 0.5) -> List[Dict]:
        """íŠ¸ë Œë“œ ë° ê´€ë ¨ì„± ê¸°ë°˜ í•„í„°ë§"""
        # ê°„ë‹¨í•œ ì ìˆ˜ ì‹œìŠ¤í…œ: ì œëª©ì— í‚¤ì›Œë“œê°€ ë§ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
        scored_articles = []

        for article in articles:
            score = 0
            title_lower = article['title'].lower()

            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            for keyword in self.keywords:
                if keyword.lower() in title_lower:
                    score += 1

            # ìµœì‹ ì„± ì ìˆ˜ (ì¶”ê°€ ê°€ëŠ¥)
            score += 0.5

            article['relevance_score'] = score
            if score >= min_relevance:
                scored_articles.append(article)

        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        scored_articles.sort(key=lambda x: x['relevance_score'], reverse=True)
        return scored_articles

    def save_to_json(self, articles: List[Dict], filename: str = "collected_news.json"):
        """ìˆ˜ì§‘ëœ ë‰´ìŠ¤ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                'collected_at': datetime.now().isoformat(),
                'total_count': len(articles),
                'articles': articles
            }, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ {filename}ì— ì €ì¥ ì™„ë£Œ")

    @staticmethod
    def _clean_html(text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        return BeautifulSoup(text, 'html.parser').get_text()


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    collector = NewsCollector()

    # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
    raw_articles = collector.collect_all_news()

    # 2. íŠ¸ë Œë“œ í•„í„°ë§
    trending_articles = collector.filter_trending_news(raw_articles)

    # 3. ìƒìœ„ 5ê°œ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘
    print("\nğŸ“° ìƒìœ„ 5ê°œ íŠ¸ë Œë“œ ê¸°ì‚¬:")
    for i, article in enumerate(trending_articles[:5], 1):
        print(f"\n{i}. {article['title']}")
        print(f"   í‚¤ì›Œë“œ: {article['keyword']} | ì ìˆ˜: {article['relevance_score']}")
        print(f"   ë§í¬: {article['link']}")

        # ë³¸ë¬¸ ìˆ˜ì§‘ (ì„ íƒì )
        # content = collector.scrape_article_content(article['link'])
        # article['content'] = content[:500]  # ì²˜ìŒ 500ìë§Œ

    # 4. JSON ì €ì¥
    collector.save_to_json(trending_articles[:20])
